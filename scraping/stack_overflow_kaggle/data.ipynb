{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import html\n",
    "import re\n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/formatted_dataset.csv', nrows=3_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Question</th>\n",
       "      <th>Id</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Continuous Integration System for a Python Cod...</td>\n",
       "      <td>&lt;p&gt;I'm starting work on a hobby project with a...</td>\n",
       "      <td>535</td>\n",
       "      <td>&lt;p&gt;I have very good experiences with &lt;a href=\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Class views in Django</td>\n",
       "      <td>&lt;p&gt;&lt;a href=\"http://www.djangoproject.com/\"&gt;Dja...</td>\n",
       "      <td>742</td>\n",
       "      <td>&lt;p&gt;You can use the Django Generic Views. You c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Python and MySQL</td>\n",
       "      <td>&lt;p&gt;I can get Python to work with Postgresql bu...</td>\n",
       "      <td>766</td>\n",
       "      <td>&lt;p&gt;Take a pick at&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\"https://d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How do I use Python's itertools.groupby()?</td>\n",
       "      <td>&lt;p&gt;I haven't been able to find an understandab...</td>\n",
       "      <td>773</td>\n",
       "      <td>&lt;p&gt;Another example:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;for key,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adding a Method to an Existing Object Instance</td>\n",
       "      <td>&lt;p&gt;I've read that it is possible to add a meth...</td>\n",
       "      <td>972</td>\n",
       "      <td>&lt;p&gt;I think that the above answers missed the k...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Continuous Integration System for a Python Cod...   \n",
       "1                              Class views in Django   \n",
       "2                                   Python and MySQL   \n",
       "3         How do I use Python's itertools.groupby()?   \n",
       "4     Adding a Method to an Existing Object Instance   \n",
       "\n",
       "                                            Question   Id  \\\n",
       "0  <p>I'm starting work on a hobby project with a...  535   \n",
       "1  <p><a href=\"http://www.djangoproject.com/\">Dja...  742   \n",
       "2  <p>I can get Python to work with Postgresql bu...  766   \n",
       "3  <p>I haven't been able to find an understandab...  773   \n",
       "4  <p>I've read that it is possible to add a meth...  972   \n",
       "\n",
       "                                              Answer  \n",
       "0  <p>I have very good experiences with <a href=\"...  \n",
       "1  <p>You can use the Django Generic Views. You c...  \n",
       "2  <p>Take a pick at</p>\\n\\n<p><a href=\"https://d...  \n",
       "3  <p>Another example:</p>\\n\\n<pre><code>for key,...  \n",
       "4  <p>I think that the above answers missed the k...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = df.Answer.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if a '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'<.+?>', '', \"if a < 2: then do this thing other wise >\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = df.Answer.iloc[4]\n",
    "# get code blocks before removing tags\n",
    "start = 0\n",
    "uncommented_code_blocks = []\n",
    "for i in range(30):\n",
    "    if \"<pre><code>\" not in string[start:]:\n",
    "        break\n",
    "    front = string.index(\"<pre><code>\", start)\n",
    "    back = string.index(\"</code></pre>\", start)+13\n",
    "    sub = string[front: back]\n",
    "    start = back\n",
    "    uncommented_code_blocks.append(sub)\n",
    "\n",
    "# add # on every line\n",
    "string = f\"\\n{string}\".replace('\\n', '\\n#')\n",
    "\n",
    "# get code blocks after (should be equal lengths)\n",
    "commented_code_blocks = []\n",
    "start = 0\n",
    "for i in range(30):\n",
    "    if \"<pre><code>\" not in string[start:]:\n",
    "        break\n",
    "    front = string.index(\"#<pre><code>\", start)\n",
    "    back = string.index(\"</code></pre>\", start)+13\n",
    "    sub = string[front: back]\n",
    "    start = back\n",
    "    commented_code_blocks.append(sub)\n",
    "\n",
    "\n",
    "# replace commented code blocks with uncommented code blocks\n",
    "for i in range(len(commented_code_blocks)):\n",
    "    string = string.replace(commented_code_blocks[i], uncommented_code_blocks[i])\n",
    "\n",
    "# remove the \"#\" on lines where theres no characters\n",
    "output = []\n",
    "for line in string.split('\\n'):\n",
    "    if line == \"#\":\n",
    "        output.append(\"\")\n",
    "    else:\n",
    "        output.append(line)\n",
    "string = \"\\n\".join(output)\n",
    "\n",
    "# remove html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#<p>I think that the above answers missed the key point. </p>\n",
      "\n",
      "#<p>Let's have a class with a method:</p>\n",
      "\n",
      "<pre><code>class A(object):\n",
      "    def m(self):\n",
      "        pass\n",
      "</code></pre>\n",
      "\n",
      "#<p>Now, let's play with it in ipython:</p>\n",
      "\n",
      "<pre><code>In [2]: A.m\n",
      "Out[2]: &lt;unbound method A.m&gt;\n",
      "</code></pre>\n",
      "\n",
      "#<p>Ok, so <em>m()</em> somehow becomes an unbound method of <em>A</em>. But is it really like that?</p>\n",
      "\n",
      "<pre><code>In [5]: A.__dict__['m']\n",
      "Out[5]: &lt;function m at 0xa66b8b4&gt;\n",
      "</code></pre>\n",
      "\n",
      "#<p>It turns out that <em>m()</em> is just a function, reference to which is added to <em>A</em> class dictionary - there's no magic. Then why <em>A.m</em> gives us an unbound method? It's because the dot is not translated to a simple dictionary lookup. It's de facto a call of A.__class__.__getattribute__(A, 'm'):</p>\n",
      "\n",
      "<pre><code>In [11]: class MetaA(type):\n",
      "   ....:     def __getattribute__(self, attr_name):\n",
      "   ....:         print str(self), '-', attr_name\n",
      "\n",
      "In [12]: class A(object):\n",
      "   ....:     __metaclass__ = MetaA\n",
      "\n",
      "In [23]: A.m\n",
      "&lt;class '__main__.A'&gt; - m\n",
      "&lt;class '__main__.A'&gt; - m\n",
      "</code></pre>\n",
      "\n",
      "#<p>Now, I'm not sure out of the top of my head why the last line is printed twice, but still it's clear what's going on there.</p>\n",
      "\n",
      "#<p>Now, what the default __getattribute__ does is that it checks if the attribute is a so-called <a href=\"http://docs.python.org/reference/datamodel.html#implementing-descriptors\" rel=\"nofollow\">descriptor</a> or not, i.e. if it implements a special __get__ method. If it implements that method, then what is returned is the result of calling that __get__ method. Going back to the first version of our <em>A</em> class, this is what we have:</p>\n",
      "\n",
      "<pre><code>In [28]: A.__dict__['m'].__get__(None, A)\n",
      "Out[28]: &lt;unbound method A.m&gt;\n",
      "</code></pre>\n",
      "\n",
      "#<p>And because Python functions implement the descriptor protocol, if they are called on behalf of an object, they bind themselves to that object in their __get__ method.</p>\n",
      "\n",
      "#<p>Ok, so how to add a method to an existing object? Assuming you don't mind patching class, it's as simple as:</p>\n",
      "\n",
      "<pre><code>B.m = m\n",
      "</code></pre>\n",
      "\n",
      "#<p>Then <em>B.m</em> \"becomes\" an unbound method, thanks to the descriptor magic.</p>\n",
      "\n",
      "#<p>And if you want to add a method just to a single object, then you have to emulate the machinery yourself, by using types.MethodType:</p>\n",
      "\n",
      "<pre><code>b.m = types.MethodType(m, b)\n",
      "</code></pre>\n",
      "\n",
      "#<p>By the way:</p>\n",
      "\n",
      "<pre><code>In [2]: A.m\n",
      "Out[2]: &lt;unbound method A.m&gt;\n",
      "\n",
      "In [59]: type(A.m)\n",
      "Out[59]: &lt;type 'instancemethod'&gt;\n",
      "\n",
      "In [60]: type(b.m)\n",
      "Out[60]: &lt;type 'instancemethod'&gt;\n",
      "\n",
      "In [61]: types.MethodType\n",
      "Out[61]: &lt;type 'instancemethod'&gt;\n",
      "</code></pre>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<p>Another example:</p>\\n\\n<pre><code>for key, igroup in itertools.groupby(xrange(12), lambda x: x // 5):\\n    print key, list(igroup)\\n</code></pre>\\n\\n<p>results in</p>\\n\\n<pre><code>0 [0, 1, 2, 3, 4]\\n1 [5, 6, 7, 8, 9]\\n2 [10, 11]\\n</code></pre>\\n\\n<p>Note that igroup is an iterator (a sub-iterator as the documentation calls it).</p>\\n\\n<p>This is useful for chunking a generator:</p>\\n\\n<pre><code>def chunker(items, chunk_size):\\n    '''Group items in chunks of chunk_size'''\\n    for _key, group in itertools.groupby(enumerate(items), lambda x: x[0] // chunk_size):\\n        yield (g[1] for g in group)\\n\\nwith open('file.txt') as fobj:\\n    for chunk in chunker(fobj):\\n        process(chunk)\\n</code></pre>\\n\\n<p>Another example of groupby - when the keys are not sorted.  In the following example, items in xx are grouped by values in yy.  In this case, one set of zeros is output first, followed by a set of ones, followed again by a set of zeros.</p>\\n\\n<pre><code>xx = range(10)\\nyy = [0, 0, 0, 1, 1, 1, 0, 0, 0, 0]\\nfor group in itertools.groupby(iter(xx), lambda x: yy[x]):\\n    print group[0], list(group[1])\\n</code></pre>\\n\\n<p>Produces:</p>\\n\\n<pre><code>0 [0, 1, 2]\\n1 [3, 4, 5]\\n0 [6, 7, 8, 9]\\n</code></pre>\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>Another example:</p>\n",
      "\n",
      "<pre><code>for key, igroup in itertools.groupby(xrange(12), lambda x: x // 5):\n",
      "    print key, list(igroup)\n",
      "</code></pre>\n",
      "\n",
      "<p>results in</p>\n",
      "\n",
      "<pre><code>0 [0, 1, 2, 3, 4]\n",
      "1 [5, 6, 7, 8, 9]\n",
      "2 [10, 11]\n",
      "</code></pre>\n",
      "\n",
      "<p>Note that igroup is an iterator (a sub-iterator as the documentation calls it).</p>\n",
      "\n",
      "<p>This is useful for chunking a generator:</p>\n",
      "\n",
      "<pre><code>def chunker(items, chunk_size):\n",
      "    '''Group items in chunks of chunk_size'''\n",
      "    for _key, group in itertools.groupby(enumerate(items), lambda x: x[0] // chunk_size):\n",
      "        yield (g[1] for g in group)\n",
      "\n",
      "with open('file.txt') as fobj:\n",
      "    for chunk in chunker(fobj):\n",
      "        process(chunk)\n",
      "</code></pre>\n",
      "\n",
      "<p>Another example of groupby - when the keys are not sorted.  In the following example, items in xx are grouped by values in yy.  In this case, one set of zeros is output first, followed by a set of ones, followed again by a set of zeros.</p>\n",
      "\n",
      "<pre><code>xx = range(10)\n",
      "yy = [0, 0, 0, 1, 1, 1, 0, 0, 0, 0]\n",
      "for group in itertools.groupby(iter(xx), lambda x: yy[x]):\n",
      "    print group[0], list(group[1])\n",
      "</code></pre>\n",
      "\n",
      "<p>Produces:</p>\n",
      "\n",
      "<pre><code>0 [0, 1, 2]\n",
      "1 [3, 4, 5]\n",
      "0 [6, 7, 8, 9]\n",
      "</code></pre>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.Answer.iloc[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(html_string):\n",
    "    html_string = html.unescape(html_string)\n",
    "    html_string = re.sub(r'<.+?>', '', html_string)\n",
    "    html_string = html_string.replace('\\r', '')\n",
    "    return html_string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format for search\n",
    "qs = []\n",
    "corpus = []\n",
    "q2a = {}\n",
    "for i in range(df.shape[0]):\n",
    "    title = clean_html(df.Title.iloc[i])\n",
    "    question = clean_html(df.Question.iloc[i])\n",
    "    corp_entry = f\"{title}\\n{question}\"\n",
    "    corpus.append(corp_entry)\n",
    "\n",
    "    question = title + ' ' + question\n",
    "    #TODO: replace puncuation, =, <, >, \" and brackets with spaces, will be better for tokinization\n",
    "    #TODO: replace any more than 1 space with many spaces\n",
    "    question = question.replace('\\n', \" \").replace('.', \" \").replace('?', \" \").replace('!', \" \").replace('(', \" \").replace(')', \" \")\n",
    "    question = question.replace('[', \" \").replace(']', \" \").replace('_', \" \").replace('  ', ' ').strip().lower()\n",
    "    qs.append(question.split(\" \"))\n",
    "\n",
    "    answer = clean_html(df.Answer.iloc[i]).replace('  ', ' ').strip()\n",
    "    q2a[corp_entry] = answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25Okapi(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents(query, corpus, n=1):\n",
    "    tokenized_query = query.lower().split(' ')\n",
    "    return bm25.get_top_n(tokenized_query, corpus, n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = get_documents(\"How do I drop duplicates from a pandas column?\", corpus, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"pandas drop_duplicates using comparison function\\nIs it somehow possible to use pandas.drop_duplicates with a comparison operator which compares two objects in a particular column in order to identify duplicates? If not, what is the alternative?\\n\\nHere is an example where it could be used:\\n\\nI have a pandas DataFrame which has lists as values in a particular column and I would like to have duplicates removed based on column A\\n\\nimport pandas as pd\\n\\ndf = pd.DataFrame( {'A': [[1,2],[2,3],[1,2]]} )\\nprint df\\n\\n\\ngiving me\\n\\n        A\\n0  [1, 2]\\n1  [2, 3]\\n2  [1, 2]\\n\\n\\nUsing pandas.drop_duplicates\\n\\ndf.drop_duplicates( 'A' )\\n\\n\\ngives me a TypeError\\n\\n[...]\\nTypeError: type object argument after * must be a sequence, not itertools.imap\\n\\n\\nHowever, my desired result is\\n\\n        A\\n0  [1, 2]\\n1  [2, 3]\\n\\n\\nMy comparison function would be here:\\n\\ndef cmp(x,y):\\n    return x==y\\n\\n\\nBut in principle it could be something else, e.g.,\\n\\ndef cmp(x,y):\\n    return x==y and len(x)>1\\n\\n\\nHow can I remove duplicates based on the comparison function in a efficient way?\\n\\nEven more, what could I do if I had more columns to compare using a different comparison function, respectively?\",\n",
       " \"Python Pandas_How to select data after using drop_duplicates()?\\nI am learning python pandas to processing data. \\n\\n\\nI firstly using drop_duplicates() method to treat db_new and get a;\\nThen I'd like to find what kind of data in a using print;\\nI try to find if a data is in a using for...in, but I found that even the data in a can not be find in it, why?\\n\\n\\n\\n\\na = db_new.iloc[:i,4:5].drop_duplicates()\\nprint a\\nfor x in a:\\n    print x**\\n\\n\\n\\nI try to use for in to find what can get in a. I only get E, which is the column index. Do you know why this happen?\",\n",
       " \"Python pandas drops duplicates in wrong order\\nWhen running drop duplicates on python pandas there seems to be a bug which causes the DataFrame to be sorted in the wrong order.\\n\\nSpecifically, I was trying to provide two columns to perform the drop duplicates on. Instead of:\\n\\ndf.drop_duplicates(['a', 'b'], inplace = True)\\n\\n\\nI had:\\n\\ndf.drop_duplicates('a', 'b', inplace = True)\\n\\n\\nWhich I think caused the problem as it disappeared with the addition of square brackets.\\n\\nI don't understand why this: a) doesn't bug out for incorrectly defined inputs, b) changes the order of what is dropped and kept.\"]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Option 1\\n\\ndf[~pd.DataFrame(df.A.values.tolist()).duplicated()]\\n\\n\\n\\n\\nOption 2\\n\\ndf[~df.A.apply(pd.Series).duplicated()]'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2a[doc[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas drop_duplicates using comparison function\n",
      "Is it somehow possible to use pandas.drop_duplicates with a comparison operator which compares two objects in a particular column in order to identify duplicates? If not, what is the alternative?\n",
      "\n",
      "Here is an example where it could be used:\n",
      "\n",
      "I have a pandas DataFrame which has lists as values in a particular column and I would like to have duplicates removed based on column A\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "df = pd.DataFrame( {'A': [[1,2],[2,3],[1,2]]} )\n",
      "print df\n",
      "\n",
      "\n",
      "giving me\n",
      "\n",
      "        A\n",
      "0  [1, 2]\n",
      "1  [2, 3]\n",
      "2  [1, 2]\n",
      "\n",
      "\n",
      "Using pandas.drop_duplicates\n",
      "\n",
      "df.drop_duplicates( 'A' )\n",
      "\n",
      "\n",
      "gives me a TypeError\n",
      "\n",
      "[...]\n",
      "TypeError: type object argument after * must be a sequence, not itertools.imap\n",
      "\n",
      "\n",
      "However, my desired result is\n",
      "\n",
      "        A\n",
      "0  [1, 2]\n",
      "1  [2, 3]\n",
      "\n",
      "\n",
      "My comparison function would be here:\n",
      "\n",
      "def cmp(x,y):\n",
      "    return x==y\n",
      "\n",
      "\n",
      "But in principle it could be something else, e.g.,\n",
      "\n",
      "def cmp(x,y):\n",
      "    return x==y and len(x)>1\n",
      "\n",
      "\n",
      "How can I remove duplicates based on the comparison function in a efficient way?\n",
      "\n",
      "Even more, what could I do if I had more columns to compare using a different comparison function, respectively?\n",
      "\n",
      "Answer:\n",
      "\n",
      " Option 1\n",
      "\n",
      "df[~pd.DataFrame(df.A.values.tolist()).duplicated()]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Option 2\n",
      "\n",
      "df[~df.A.apply(pd.Series).duplicated()]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Python Pandas_How to select data after using drop_duplicates()?\n",
      "I am learning python pandas to processing data. \n",
      "\n",
      "\n",
      "I firstly using drop_duplicates() method to treat db_new and get a;\n",
      "Then I'd like to find what kind of data in a using print;\n",
      "I try to find if a data is in a using for...in, but I found that even the data in a can not be find in it, why?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "a = db_new.iloc[:i,4:5].drop_duplicates()\n",
      "print a\n",
      "for x in a:\n",
      "    print x**\n",
      "\n",
      "\n",
      "\n",
      "I try to use for in to find what can get in a. I only get E, which is the column index. Do you know why this happen?\n",
      "\n",
      "\n",
      "Answer:\n",
      "\n",
      " Here a is a dataframe, so when you iterate over a you iterate over column names, hence the result, E.\n",
      "\n",
      "If you want to iterate over values, you need to make a a series, which you can do using squeeze:\n",
      "\n",
      "for x in a.squeeze():\n",
      "  print x\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Python pandas drops duplicates in wrong order\n",
      "When running drop duplicates on python pandas there seems to be a bug which causes the DataFrame to be sorted in the wrong order.\n",
      "\n",
      "Specifically, I was trying to provide two columns to perform the drop duplicates on. Instead of:\n",
      "\n",
      "df.drop_duplicates(['a', 'b'], inplace = True)\n",
      "\n",
      "\n",
      "I had:\n",
      "\n",
      "df.drop_duplicates('a', 'b', inplace = True)\n",
      "\n",
      "\n",
      "Which I think caused the problem as it disappeared with the addition of square brackets.\n",
      "\n",
      "I don't understand why this: a) doesn't bug out for incorrectly defined inputs, b) changes the order of what is dropped and kept.\n",
      "\n",
      "Answer:\n",
      "\n",
      " The docs for drop_duplicates say the arguments are:\n",
      "\n",
      "\n",
      " \n",
      " subset : column label or sequence of labels, optional\n",
      " Only consider certain columns for identifying duplicates, by default use all of the columns\n",
      " take_last : boolean, default False\n",
      " Take the last observed row in a row. Defaults to the first row\n",
      " inplace : boolean, default False\n",
      " Whether to drop duplicates in place or to return a copy\n",
      " cols : kwargs only argument of subset [deprecated]\n",
      " \n",
      "\n",
      "\n",
      "So, in your wring call, it probably used b for the take_last, which it evaluated as a Boolean True. This is standard practice in Python (checking for wrong inputs is not comprehensive).\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "doc = get_documents(\"How do I drop duplicates from a pandas column?\", corpus, 3)\n",
    "\n",
    "for d in doc:\n",
    "    print(d)\n",
    "    print('Answer:')\n",
    "    print('\\n', q2a[d])\n",
    "    print(\"-\"* 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Easy way to determine a nesting level of nested tuples, in Python\n",
      "Is there an easy way to determine the nesting level (without dependence on Python's recursion limit) of t (representing recombining binomial tree)?\n",
      "\n",
      "t = (4, (3, 5, (2, 4, 6, (1, 3, 5, 7))))\n",
      "\n",
      "\n",
      "Note that without a priori knowledge of depth of t, a routine may face recursion limit, which is set by sys.setrecursionlimit(n) and viewed by sys.getrecursionlimit(). Still, setting recursion limit very high before hand, may not be sufficient, thereby generating an error \n",
      "\n",
      "`RecursionError: maximum recursion depth exceeded while calling a Python object`. \n",
      "\n",
      "\n",
      "The following generates a larger (deeper) t: \n",
      "\n",
      "t = tuple(tuple(range(k)) for k in range(1,200))`\n",
      "\n",
      "\n",
      "I guess these may work (have not worked out the details):\n",
      "\n",
      "\n",
      "one can convert t to string and count number of opening brackets\n",
      "If flattened tuple has size $N$, then the depth has size of the positive quadratic root of $n(n+1)/2=N$, that is $n=(-1+\\sqrt(1+8N))/2$\n",
      "iteratively peel (and count) the outer container until deepest nesting\n",
      "any others?\n",
      "\n",
      "\n",
      "P.S. Any ideas why in-line TeX is not rendering in my question? Testing: $N$\n",
      "\n",
      "Answer:\n",
      "\n",
      " You can convert the recursion function into a stack that you manage yourself, e.g.\n",
      "\n",
      "t = (4, (3, 5, (2, 4, 6, (1, 3, 5, 7))))\n",
      "\n",
      "def depth(t):\n",
      "  max_depth = 0\n",
      "  A = [(t,max_depth)]\n",
      "  while A:\n",
      "    x,depth = A.pop()\n",
      "    if isinstance(x, (list, tuple)):\n",
      "      for a in x:\n",
      "        A.append((a,depth+1))\n",
      "    else:\n",
      "      max_depth = max(max_depth,depth)\n",
      "  return max_depth\n",
      "\n",
      "print depth(1) # Prints 0\n",
      "print depth((1,1)) # Prints 1\n",
      "print depth(t) # Prints 4\n",
      "\n",
      "\n",
      "This is not a recursive function so will not hit the recursion limit.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fastest Search Through Random Nested Data\n",
      "[\n",
      "   [\n",
      "      [2,33,64,276,1],\n",
      "      [234,5,234,7,34,36,7,2],\n",
      "      []\n",
      "   ]\n",
      "   [\n",
      "      [2,4,5]\n",
      "   ]\n",
      "   .\n",
      "   .\n",
      "   .\n",
      "   etc\n",
      "]\n",
      "\n",
      "\n",
      "I'm not looking for an exact solution to this, as the structure above is just an example. I'm trying to search for an ID that can be nested several levels deep within a group of IDs ordered randomly.\n",
      "\n",
      "Currently I'm just doing a linear search which takes a few minutes to get a result when each of the deepest levels has a couple hundred of IDs. I was wondering if anyone could suggest a faster algorithm for searching through multiple levels of random data? I am doing this in Python if that matters.\n",
      "\n",
      "Note: The IDs are always at the deepest level and the number of levels is consistent for each branch down. Not sure if that matters or not.\n",
      "\n",
      "Also to clarify the data points are unique and cannot be repeated. My example has some repeats because I was just smashing the keyboard.\n",
      "\n",
      "Answer:\n",
      "\n",
      " The fastest search through random data is linear. Pretending your data isn't nested, it's still random, so even flattening it won't help.\n",
      "\n",
      "To decrease the time complexity, you can increase the space complexity -- keep a dict containing IDs as keys and whatever information you want (possibly a list of indices pointing to the list containing the ID at each level), and update it every time you create/update/delete an element.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sum a nested list of a nested list of a nested list of integers\n",
      "Given a Python list whose elements are either integers or lists of integers (only we don't know how deep the nesting goes), how can we find the sum of each individual integer within the list?\n",
      "\n",
      "It's fairly straightforward to find the sum of a list whose nesting only goes one level deep, e.g.\n",
      "\n",
      "[1, [1, 2, 3]]\n",
      "# sum is 7\n",
      "\n",
      "\n",
      "But what if the nesting goes two, three, or more levels deep?\n",
      "\n",
      "[1, [1, [2, 3]]]\n",
      "# two levels deep\n",
      "\n",
      "[1, [1, [2, [3]]]]\n",
      "# three levels deep\n",
      "\n",
      "\n",
      "The sum in each of the above cases is the same (i.e. 7). I think the best approach is using recursion where the base case is a list with a single integer element, but beyond that I'm stuck.\n",
      "\n",
      "Answer:\n",
      "\n",
      " You can use this recursive solution:\n",
      "\n",
      "from collections import Iterable\n",
      "def flatten(collection):\n",
      " for element in collection:\n",
      "  if isinstance(element, Iterable):\n",
      "   for x in flatten(element):\n",
      "    yield x\n",
      "  else:\n",
      "   yield element\n",
      "\n",
      "\n",
      "Demo:\n",
      "\n",
      ">>> lis = [1, [1, [2, [3]]]]\n",
      ">>> sum(flatten(lis))\n",
      "7\n",
      ">>> lis = [1, [1, 2, 3]]\n",
      ">>> sum(flatten(lis))\n",
      "7\n",
      ">>> lis = [1, [1, [2, 3]]]\n",
      ">>> sum(flatten(lis))\n",
      "7\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "q = \"\"\"\n",
    "Input to this function is a string represented multiple groups for nested parentheses separated by spaces.\n",
    "    For each of the group, output the deepest level of nesting of parentheses.\n",
    "    E.g. (()()) has maximum two levels of nesting while ((())) has three.\n",
    "\"\"\"\n",
    "doc = get_documents(q, corpus, 3)\n",
    "\n",
    "for d in doc:\n",
    "    print(d)\n",
    "    print('Answer:')\n",
    "    print('\\n', q2a[d])\n",
    "    print(\"-\"* 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./bm25-files/corpus.pkl', 'rb') as f:\n",
    "    corpus = pickle.load(f)\n",
    "    \n",
    "with open('./bm25-files/q2a.pkl', 'rb') as f:\n",
    "    q2a = pickle.load(f)\n",
    "\n",
    "with open('./bm25-files/bm25.pkl', 'rb') as f:\n",
    "    bm25 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents(query, corpus, n=1):\n",
    "    tokenized_query = query.lower().split(' ')\n",
    "    return bm25.get_top_n(tokenized_query, corpus, n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do you remove duplicates from a list in whilst preserving order?\n",
      "Is there a built-in that removes duplicates from list in Python, whilst preserving order? I know that I can use a set to remove duplicates, but that destroys the original order. I also know that I can roll my own like this:\n",
      "\n",
      "def uniq(input):\n",
      "  output = []\n",
      "  for x in input:\n",
      "    if x not in output:\n",
      "      output.append(x)\n",
      "  return output\n",
      "\n",
      "\n",
      "(Thanks to unwind for that code sample.)\n",
      "\n",
      "But I'd like to avail myself of a built-in or a more Pythonic idiom if possible.\n",
      "\n",
      "Related question: In Python, what is the fastest algorithm for removing duplicates from a list so that all elements are unique while preserving order?\n",
      "Answer:\n",
      "\n",
      " #Important Edit 2015\n",
      "\n",
      "#As @abarnert notes, the more_itertools library (pip install more_itertools) contains a unique_everseen function that is built to solve this problem without any unreadable (not seen.add) mutations in list comprehensions. This is also the fastest solution too:\n",
      "\n",
      ">>> from  more_itertools import unique_everseen\n",
      ">>> items = [1, 2, 0, 1, 3, 2]\n",
      ">>> list(unique_everseen(items))\n",
      "[1, 2, 0, 3]\n",
      "\n",
      "\n",
      "#Just one simple library import and no hacks. \n",
      "#This comes from an implementation of the itertools recipe unique_everseen which looks like:\n",
      "\n",
      "def unique_everseen(iterable, key=None):\n",
      "    \"List unique elements, preserving order. Remember all elements ever seen.\"\n",
      "    # unique_everseen('AAAABBBCCDAABBB') --> A B C D\n",
      "    # unique_everseen('ABBCcAD', str.lower) --> A B C D\n",
      "    seen = set()\n",
      "    seen_add = seen.add\n",
      "    if key is None:\n",
      "        for element in filterfalse(seen.__contains__, iterable):\n",
      "            seen_add(element)\n",
      "            yield element\n",
      "    else:\n",
      "        for element in iterable:\n",
      "            k = key(element)\n",
      "            if k not in seen:\n",
      "                seen_add(k)\n",
      "                yield element\n",
      "\n",
      "\n",
      "#\n",
      "\n",
      "#In Python 2.7+ the accepted common idiom (this works but isn't optimized for speed, i would now use unique_everseen) for this uses collections.OrderedDict:\n",
      "\n",
      "#Runtime: O(N)\n",
      "\n",
      ">>> from collections import OrderedDict\n",
      ">>> items = [1, 2, 0, 1, 3, 2]\n",
      ">>> list(OrderedDict.fromkeys(items))\n",
      "[1, 2, 0, 3]\n",
      "\n",
      "\n",
      "#This looks much nicer than:\n",
      "\n",
      "seen = set()\n",
      "[x for x in seq if x not in seen and not seen.add(x)]\n",
      "\n",
      "\n",
      "#and doesn't utilize the ugly hack:\n",
      "\n",
      "not seen.add(x)\n",
      "\n",
      "\n",
      "#which relies on the fact that set.add is an in-place method that always returns None so not None evaluates to True. \n",
      "\n",
      "#Note however that the hack solution is faster in raw speed though it has the same runtime complexity O(N).\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Parsing a file with column data in Python\n",
      "I have a file that contains the symbol table details.Its in the form of rows and columns.\n",
      "\n",
      "I need to extract first and last column.\n",
      "\n",
      "How can I do that?\n",
      "Answer:\n",
      "\n",
      " #According to the update, columns are seperated with space. So it would be:\n",
      "\n",
      "rawfile = open('details.Its', 'r')\n",
      "table = [line.rstrip().split() for line in rawfile.readlines()]\n",
      "\n",
      "newtable = [[line[0]]+[line[-1]] for line in table]\n",
      "print(newtable)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "How can I use UUIDs in SQLAlchemy?\n",
      "Is there a way to define a column (primary key) as a UUID in SQLAlchemy if using PostgreSQL (Postgres)?\n",
      "Answer:\n",
      "\n",
      " #Unfortunately Backend-agnostic GUID Type from the SQLAlchemy documentation for column types does not seem to work for primary keys in SQLite database engines. Not quite as ecumenical as I was hoping for.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "doc = get_documents(\"how do I drop duplicates from a pandas column\", corpus, 3)\n",
    "\n",
    "for d in doc:\n",
    "    print(d)\n",
    "    print('Answer:')\n",
    "    print('\\n', q2a[d])\n",
    "    print(\"-\"* 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "00eb8093f170c342c464ce3c2b3e156a06d4906fbae28a587b903ec4fb1d586e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('code-generation')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
