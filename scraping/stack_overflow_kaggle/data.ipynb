{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import html\n",
    "import re\n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/formatted_dataset.csv', nrows=3_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Question</th>\n",
       "      <th>Id</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Continuous Integration System for a Python Cod...</td>\n",
       "      <td>&lt;p&gt;I'm starting work on a hobby project with a...</td>\n",
       "      <td>535</td>\n",
       "      <td>&lt;p&gt;I have very good experiences with &lt;a href=\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Class views in Django</td>\n",
       "      <td>&lt;p&gt;&lt;a href=\"http://www.djangoproject.com/\"&gt;Dja...</td>\n",
       "      <td>742</td>\n",
       "      <td>&lt;p&gt;You can use the Django Generic Views. You c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Python and MySQL</td>\n",
       "      <td>&lt;p&gt;I can get Python to work with Postgresql bu...</td>\n",
       "      <td>766</td>\n",
       "      <td>&lt;p&gt;Take a pick at&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\"https://d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How do I use Python's itertools.groupby()?</td>\n",
       "      <td>&lt;p&gt;I haven't been able to find an understandab...</td>\n",
       "      <td>773</td>\n",
       "      <td>&lt;p&gt;Another example:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;for key,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adding a Method to an Existing Object Instance</td>\n",
       "      <td>&lt;p&gt;I've read that it is possible to add a meth...</td>\n",
       "      <td>972</td>\n",
       "      <td>&lt;p&gt;I think that the above answers missed the k...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Continuous Integration System for a Python Cod...   \n",
       "1                              Class views in Django   \n",
       "2                                   Python and MySQL   \n",
       "3         How do I use Python's itertools.groupby()?   \n",
       "4     Adding a Method to an Existing Object Instance   \n",
       "\n",
       "                                            Question   Id  \\\n",
       "0  <p>I'm starting work on a hobby project with a...  535   \n",
       "1  <p><a href=\"http://www.djangoproject.com/\">Dja...  742   \n",
       "2  <p>I can get Python to work with Postgresql bu...  766   \n",
       "3  <p>I haven't been able to find an understandab...  773   \n",
       "4  <p>I've read that it is possible to add a meth...  972   \n",
       "\n",
       "                                              Answer  \n",
       "0  <p>I have very good experiences with <a href=\"...  \n",
       "1  <p>You can use the Django Generic Views. You c...  \n",
       "2  <p>Take a pick at</p>\\n\\n<p><a href=\"https://d...  \n",
       "3  <p>Another example:</p>\\n\\n<pre><code>for key,...  \n",
       "4  <p>I think that the above answers missed the k...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<p>Another example:</p>\\n\\n<pre><code>for key, igroup in itertools.groupby(xrange(12), lambda x: x // 5):\\n    print key, list(igroup)\\n</code></pre>\\n\\n<p>results in</p>\\n\\n<pre><code>0 [0, 1, 2, 3, 4]\\n1 [5, 6, 7, 8, 9]\\n2 [10, 11]\\n</code></pre>\\n\\n<p>Note that igroup is an iterator (a sub-iterator as the documentation calls it).</p>\\n\\n<p>This is useful for chunking a generator:</p>\\n\\n<pre><code>def chunker(items, chunk_size):\\n    '''Group items in chunks of chunk_size'''\\n    for _key, group in itertools.groupby(enumerate(items), lambda x: x[0] // chunk_size):\\n        yield (g[1] for g in group)\\n\\nwith open('file.txt') as fobj:\\n    for chunk in chunker(fobj):\\n        process(chunk)\\n</code></pre>\\n\\n<p>Another example of groupby - when the keys are not sorted.  In the following example, items in xx are grouped by values in yy.  In this case, one set of zeros is output first, followed by a set of ones, followed again by a set of zeros.</p>\\n\\n<pre><code>xx = range(10)\\nyy = [0, 0, 0, 1, 1, 1, 0, 0, 0, 0]\\nfor group in itertools.groupby(iter(xx), lambda x: yy[x]):\\n    print group[0], list(group[1])\\n</code></pre>\\n\\n<p>Produces:</p>\\n\\n<pre><code>0 [0, 1, 2]\\n1 [3, 4, 5]\\n0 [6, 7, 8, 9]\\n</code></pre>\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Answer.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = df.Answer.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<p>Another example:</p>\\n\\n<pre><code>for key, igroup in itertools.groupby(xrange(12), lambda x: x // 5):\\n    print key, list(igroup)\\n</code></pre>\\n\\n<p>results in</p>\\n\\n<pre><code>0 [0, 1, 2, 3, 4]\\n1 [5, 6, 7, 8, 9]\\n2 [10, 11]\\n</code></pre>\\n\\n<p>Note that igroup is an iterator (a sub-iterator as the documentation calls it).</p>\\n\\n<p>This is useful for chunking a generator:</p>\\n\\n<pre><code>def chunker(items, chunk_size):\\n    '''Group items in chunks of chunk_size'''\\n    for _key, group in itertools.groupby(enumerate(items), lambda x: x[0] // chunk_size):\\n        yield (g[1] for g in group)\\n\\nwith open('file.txt') as fobj:\\n    for chunk in chunker(fobj):\\n        process(chunk)\\n</code></pre>\\n\\n<p>Another example of groupby - when the keys are not sorted.  In the following example, items in xx are grouped by values in yy.  In this case, one set of zeros is output first, followed by a set of ones, followed again by a set of zeros.</p>\\n\\n<pre><code>xx = range(10)\\nyy = [0, 0, 0, 1, 1, 1, 0, 0, 0, 0]\\nfor group in itertools.groupby(iter(xx), lambda x: yy[x]):\\n    print group[0], list(group[1])\\n</code></pre>\\n\\n<p>Produces:</p>\\n\\n<pre><code>0 [0, 1, 2]\\n1 [3, 4, 5]\\n0 [6, 7, 8, 9]\\n</code></pre>\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>Another example:</p>\n",
      "\n",
      "<pre><code>for key, igroup in itertools.groupby(xrange(12), lambda x: x // 5):\n",
      "    print key, list(igroup)\n",
      "</code></pre>\n",
      "\n",
      "<p>results in</p>\n",
      "\n",
      "<pre><code>0 [0, 1, 2, 3, 4]\n",
      "1 [5, 6, 7, 8, 9]\n",
      "2 [10, 11]\n",
      "</code></pre>\n",
      "\n",
      "<p>Note that igroup is an iterator (a sub-iterator as the documentation calls it).</p>\n",
      "\n",
      "<p>This is useful for chunking a generator:</p>\n",
      "\n",
      "<pre><code>def chunker(items, chunk_size):\n",
      "    '''Group items in chunks of chunk_size'''\n",
      "    for _key, group in itertools.groupby(enumerate(items), lambda x: x[0] // chunk_size):\n",
      "        yield (g[1] for g in group)\n",
      "\n",
      "with open('file.txt') as fobj:\n",
      "    for chunk in chunker(fobj):\n",
      "        process(chunk)\n",
      "</code></pre>\n",
      "\n",
      "<p>Another example of groupby - when the keys are not sorted.  In the following example, items in xx are grouped by values in yy.  In this case, one set of zeros is output first, followed by a set of ones, followed again by a set of zeros.</p>\n",
      "\n",
      "<pre><code>xx = range(10)\n",
      "yy = [0, 0, 0, 1, 1, 1, 0, 0, 0, 0]\n",
      "for group in itertools.groupby(iter(xx), lambda x: yy[x]):\n",
      "    print group[0], list(group[1])\n",
      "</code></pre>\n",
      "\n",
      "<p>Produces:</p>\n",
      "\n",
      "<pre><code>0 [0, 1, 2]\n",
      "1 [3, 4, 5]\n",
      "0 [6, 7, 8, 9]\n",
      "</code></pre>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.Answer.iloc[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(html_string):\n",
    "    html_string = html.unescape(html_string)\n",
    "    html_string = re.sub(r'<.+?>', '', html_string)\n",
    "    html_string = html_string.replace('\\r', '')\n",
    "    return html_string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format for search\n",
    "qs = []\n",
    "corpus = []\n",
    "q2a = {}\n",
    "for i in range(df.shape[0]):\n",
    "    title = clean_html(df.Title.iloc[i])\n",
    "    question = clean_html(df.Question.iloc[i])\n",
    "    corp_entry = f\"{title}\\n{question}\"\n",
    "    corpus.append(corp_entry)\n",
    "\n",
    "    question = title + ' ' + question\n",
    "    #TODO: replace puncuation, =, <, >, \" and brackets with spaces, will be better for tokinization\n",
    "    #TODO: replace any more than 1 space with many spaces\n",
    "    question = question.replace('\\n', \" \").replace('.', \" \").replace('?', \" \").replace('!', \" \").replace('(', \" \").replace(')', \" \")\n",
    "    question = question.replace('[', \" \").replace(']', \" \").replace('_', \" \").replace('  ', ' ').strip().lower()\n",
    "    qs.append(question.split(\" \"))\n",
    "\n",
    "    answer = clean_html(df.Answer.iloc[i]).replace('  ', ' ').strip()\n",
    "    q2a[corp_entry] = answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25Okapi(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents(query, corpus, n=1):\n",
    "    tokenized_query = query.lower().split(' ')\n",
    "    return bm25.get_top_n(tokenized_query, corpus, n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = get_documents(\"How do I drop duplicates from a pandas column?\", corpus, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"pandas drop_duplicates using comparison function\\nIs it somehow possible to use pandas.drop_duplicates with a comparison operator which compares two objects in a particular column in order to identify duplicates? If not, what is the alternative?\\n\\nHere is an example where it could be used:\\n\\nI have a pandas DataFrame which has lists as values in a particular column and I would like to have duplicates removed based on column A\\n\\nimport pandas as pd\\n\\ndf = pd.DataFrame( {'A': [[1,2],[2,3],[1,2]]} )\\nprint df\\n\\n\\ngiving me\\n\\n        A\\n0  [1, 2]\\n1  [2, 3]\\n2  [1, 2]\\n\\n\\nUsing pandas.drop_duplicates\\n\\ndf.drop_duplicates( 'A' )\\n\\n\\ngives me a TypeError\\n\\n[...]\\nTypeError: type object argument after * must be a sequence, not itertools.imap\\n\\n\\nHowever, my desired result is\\n\\n        A\\n0  [1, 2]\\n1  [2, 3]\\n\\n\\nMy comparison function would be here:\\n\\ndef cmp(x,y):\\n    return x==y\\n\\n\\nBut in principle it could be something else, e.g.,\\n\\ndef cmp(x,y):\\n    return x==y and len(x)>1\\n\\n\\nHow can I remove duplicates based on the comparison function in a efficient way?\\n\\nEven more, what could I do if I had more columns to compare using a different comparison function, respectively?\",\n",
       " \"Python Pandas_How to select data after using drop_duplicates()?\\nI am learning python pandas to processing data. \\n\\n\\nI firstly using drop_duplicates() method to treat db_new and get a;\\nThen I'd like to find what kind of data in a using print;\\nI try to find if a data is in a using for...in, but I found that even the data in a can not be find in it, why?\\n\\n\\n\\n\\na = db_new.iloc[:i,4:5].drop_duplicates()\\nprint a\\nfor x in a:\\n    print x**\\n\\n\\n\\nI try to use for in to find what can get in a. I only get E, which is the column index. Do you know why this happen?\",\n",
       " \"Python pandas drops duplicates in wrong order\\nWhen running drop duplicates on python pandas there seems to be a bug which causes the DataFrame to be sorted in the wrong order.\\n\\nSpecifically, I was trying to provide two columns to perform the drop duplicates on. Instead of:\\n\\ndf.drop_duplicates(['a', 'b'], inplace = True)\\n\\n\\nI had:\\n\\ndf.drop_duplicates('a', 'b', inplace = True)\\n\\n\\nWhich I think caused the problem as it disappeared with the addition of square brackets.\\n\\nI don't understand why this: a) doesn't bug out for incorrectly defined inputs, b) changes the order of what is dropped and kept.\"]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Option 1\\n\\ndf[~pd.DataFrame(df.A.values.tolist()).duplicated()]\\n\\n\\n\\n\\nOption 2\\n\\ndf[~df.A.apply(pd.Series).duplicated()]'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2a[doc[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas drop_duplicates using comparison function\n",
      "Is it somehow possible to use pandas.drop_duplicates with a comparison operator which compares two objects in a particular column in order to identify duplicates? If not, what is the alternative?\n",
      "\n",
      "Here is an example where it could be used:\n",
      "\n",
      "I have a pandas DataFrame which has lists as values in a particular column and I would like to have duplicates removed based on column A\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "df = pd.DataFrame( {'A': [[1,2],[2,3],[1,2]]} )\n",
      "print df\n",
      "\n",
      "\n",
      "giving me\n",
      "\n",
      "        A\n",
      "0  [1, 2]\n",
      "1  [2, 3]\n",
      "2  [1, 2]\n",
      "\n",
      "\n",
      "Using pandas.drop_duplicates\n",
      "\n",
      "df.drop_duplicates( 'A' )\n",
      "\n",
      "\n",
      "gives me a TypeError\n",
      "\n",
      "[...]\n",
      "TypeError: type object argument after * must be a sequence, not itertools.imap\n",
      "\n",
      "\n",
      "However, my desired result is\n",
      "\n",
      "        A\n",
      "0  [1, 2]\n",
      "1  [2, 3]\n",
      "\n",
      "\n",
      "My comparison function would be here:\n",
      "\n",
      "def cmp(x,y):\n",
      "    return x==y\n",
      "\n",
      "\n",
      "But in principle it could be something else, e.g.,\n",
      "\n",
      "def cmp(x,y):\n",
      "    return x==y and len(x)>1\n",
      "\n",
      "\n",
      "How can I remove duplicates based on the comparison function in a efficient way?\n",
      "\n",
      "Even more, what could I do if I had more columns to compare using a different comparison function, respectively?\n",
      "\n",
      "Answer:\n",
      "\n",
      " Option 1\n",
      "\n",
      "df[~pd.DataFrame(df.A.values.tolist()).duplicated()]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Option 2\n",
      "\n",
      "df[~df.A.apply(pd.Series).duplicated()]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Python Pandas_How to select data after using drop_duplicates()?\n",
      "I am learning python pandas to processing data. \n",
      "\n",
      "\n",
      "I firstly using drop_duplicates() method to treat db_new and get a;\n",
      "Then I'd like to find what kind of data in a using print;\n",
      "I try to find if a data is in a using for...in, but I found that even the data in a can not be find in it, why?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "a = db_new.iloc[:i,4:5].drop_duplicates()\n",
      "print a\n",
      "for x in a:\n",
      "    print x**\n",
      "\n",
      "\n",
      "\n",
      "I try to use for in to find what can get in a. I only get E, which is the column index. Do you know why this happen?\n",
      "\n",
      "\n",
      "Answer:\n",
      "\n",
      " Here a is a dataframe, so when you iterate over a you iterate over column names, hence the result, E.\n",
      "\n",
      "If you want to iterate over values, you need to make a a series, which you can do using squeeze:\n",
      "\n",
      "for x in a.squeeze():\n",
      "  print x\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Python pandas drops duplicates in wrong order\n",
      "When running drop duplicates on python pandas there seems to be a bug which causes the DataFrame to be sorted in the wrong order.\n",
      "\n",
      "Specifically, I was trying to provide two columns to perform the drop duplicates on. Instead of:\n",
      "\n",
      "df.drop_duplicates(['a', 'b'], inplace = True)\n",
      "\n",
      "\n",
      "I had:\n",
      "\n",
      "df.drop_duplicates('a', 'b', inplace = True)\n",
      "\n",
      "\n",
      "Which I think caused the problem as it disappeared with the addition of square brackets.\n",
      "\n",
      "I don't understand why this: a) doesn't bug out for incorrectly defined inputs, b) changes the order of what is dropped and kept.\n",
      "\n",
      "Answer:\n",
      "\n",
      " The docs for drop_duplicates say the arguments are:\n",
      "\n",
      "\n",
      " \n",
      " subset : column label or sequence of labels, optional\n",
      " Only consider certain columns for identifying duplicates, by default use all of the columns\n",
      " take_last : boolean, default False\n",
      " Take the last observed row in a row. Defaults to the first row\n",
      " inplace : boolean, default False\n",
      " Whether to drop duplicates in place or to return a copy\n",
      " cols : kwargs only argument of subset [deprecated]\n",
      " \n",
      "\n",
      "\n",
      "So, in your wring call, it probably used b for the take_last, which it evaluated as a Boolean True. This is standard practice in Python (checking for wrong inputs is not comprehensive).\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "doc = get_documents(\"How do I drop duplicates from a pandas column?\", corpus, 3)\n",
    "\n",
    "for d in doc:\n",
    "    print(d)\n",
    "    print('Answer:')\n",
    "    print('\\n', q2a[d])\n",
    "    print(\"-\"* 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Easy way to determine a nesting level of nested tuples, in Python\n",
      "Is there an easy way to determine the nesting level (without dependence on Python's recursion limit) of t (representing recombining binomial tree)?\n",
      "\n",
      "t = (4, (3, 5, (2, 4, 6, (1, 3, 5, 7))))\n",
      "\n",
      "\n",
      "Note that without a priori knowledge of depth of t, a routine may face recursion limit, which is set by sys.setrecursionlimit(n) and viewed by sys.getrecursionlimit(). Still, setting recursion limit very high before hand, may not be sufficient, thereby generating an error \n",
      "\n",
      "`RecursionError: maximum recursion depth exceeded while calling a Python object`. \n",
      "\n",
      "\n",
      "The following generates a larger (deeper) t: \n",
      "\n",
      "t = tuple(tuple(range(k)) for k in range(1,200))`\n",
      "\n",
      "\n",
      "I guess these may work (have not worked out the details):\n",
      "\n",
      "\n",
      "one can convert t to string and count number of opening brackets\n",
      "If flattened tuple has size $N$, then the depth has size of the positive quadratic root of $n(n+1)/2=N$, that is $n=(-1+\\sqrt(1+8N))/2$\n",
      "iteratively peel (and count) the outer container until deepest nesting\n",
      "any others?\n",
      "\n",
      "\n",
      "P.S. Any ideas why in-line TeX is not rendering in my question? Testing: $N$\n",
      "\n",
      "Answer:\n",
      "\n",
      " You can convert the recursion function into a stack that you manage yourself, e.g.\n",
      "\n",
      "t = (4, (3, 5, (2, 4, 6, (1, 3, 5, 7))))\n",
      "\n",
      "def depth(t):\n",
      "  max_depth = 0\n",
      "  A = [(t,max_depth)]\n",
      "  while A:\n",
      "    x,depth = A.pop()\n",
      "    if isinstance(x, (list, tuple)):\n",
      "      for a in x:\n",
      "        A.append((a,depth+1))\n",
      "    else:\n",
      "      max_depth = max(max_depth,depth)\n",
      "  return max_depth\n",
      "\n",
      "print depth(1) # Prints 0\n",
      "print depth((1,1)) # Prints 1\n",
      "print depth(t) # Prints 4\n",
      "\n",
      "\n",
      "This is not a recursive function so will not hit the recursion limit.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fastest Search Through Random Nested Data\n",
      "[\n",
      "   [\n",
      "      [2,33,64,276,1],\n",
      "      [234,5,234,7,34,36,7,2],\n",
      "      []\n",
      "   ]\n",
      "   [\n",
      "      [2,4,5]\n",
      "   ]\n",
      "   .\n",
      "   .\n",
      "   .\n",
      "   etc\n",
      "]\n",
      "\n",
      "\n",
      "I'm not looking for an exact solution to this, as the structure above is just an example. I'm trying to search for an ID that can be nested several levels deep within a group of IDs ordered randomly.\n",
      "\n",
      "Currently I'm just doing a linear search which takes a few minutes to get a result when each of the deepest levels has a couple hundred of IDs. I was wondering if anyone could suggest a faster algorithm for searching through multiple levels of random data? I am doing this in Python if that matters.\n",
      "\n",
      "Note: The IDs are always at the deepest level and the number of levels is consistent for each branch down. Not sure if that matters or not.\n",
      "\n",
      "Also to clarify the data points are unique and cannot be repeated. My example has some repeats because I was just smashing the keyboard.\n",
      "\n",
      "Answer:\n",
      "\n",
      " The fastest search through random data is linear. Pretending your data isn't nested, it's still random, so even flattening it won't help.\n",
      "\n",
      "To decrease the time complexity, you can increase the space complexity -- keep a dict containing IDs as keys and whatever information you want (possibly a list of indices pointing to the list containing the ID at each level), and update it every time you create/update/delete an element.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sum a nested list of a nested list of a nested list of integers\n",
      "Given a Python list whose elements are either integers or lists of integers (only we don't know how deep the nesting goes), how can we find the sum of each individual integer within the list?\n",
      "\n",
      "It's fairly straightforward to find the sum of a list whose nesting only goes one level deep, e.g.\n",
      "\n",
      "[1, [1, 2, 3]]\n",
      "# sum is 7\n",
      "\n",
      "\n",
      "But what if the nesting goes two, three, or more levels deep?\n",
      "\n",
      "[1, [1, [2, 3]]]\n",
      "# two levels deep\n",
      "\n",
      "[1, [1, [2, [3]]]]\n",
      "# three levels deep\n",
      "\n",
      "\n",
      "The sum in each of the above cases is the same (i.e. 7). I think the best approach is using recursion where the base case is a list with a single integer element, but beyond that I'm stuck.\n",
      "\n",
      "Answer:\n",
      "\n",
      " You can use this recursive solution:\n",
      "\n",
      "from collections import Iterable\n",
      "def flatten(collection):\n",
      " for element in collection:\n",
      "  if isinstance(element, Iterable):\n",
      "   for x in flatten(element):\n",
      "    yield x\n",
      "  else:\n",
      "   yield element\n",
      "\n",
      "\n",
      "Demo:\n",
      "\n",
      ">>> lis = [1, [1, [2, [3]]]]\n",
      ">>> sum(flatten(lis))\n",
      "7\n",
      ">>> lis = [1, [1, 2, 3]]\n",
      ">>> sum(flatten(lis))\n",
      "7\n",
      ">>> lis = [1, [1, [2, 3]]]\n",
      ">>> sum(flatten(lis))\n",
      "7\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "q = \"\"\"\n",
    "Input to this function is a string represented multiple groups for nested parentheses separated by spaces.\n",
    "    For each of the group, output the deepest level of nesting of parentheses.\n",
    "    E.g. (()()) has maximum two levels of nesting while ((())) has three.\n",
    "\"\"\"\n",
    "doc = get_documents(q, corpus, 3)\n",
    "\n",
    "for d in doc:\n",
    "    print(d)\n",
    "    print('Answer:')\n",
    "    print('\\n', q2a[d])\n",
    "    print(\"-\"* 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "00eb8093f170c342c464ce3c2b3e156a06d4906fbae28a587b903ec4fb1d586e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('code-generation')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
