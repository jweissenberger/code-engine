{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('/Users/jackweissenberger/Documents/human-eval/data/HumanEval.jsonl', 'r')\n",
    "file = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task_id': 'HumanEval/0',\n",
       " 'prompt': 'from typing import List\\n\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n',\n",
       " 'entry_point': 'has_close_elements',\n",
       " 'canonical_solution': '    for idx, elem in enumerate(numbers):\\n        for idx2, elem2 in enumerate(numbers):\\n            if idx != idx2:\\n                distance = abs(elem - elem2)\\n                if distance < threshold:\\n                    return True\\n\\n    return False\\n',\n",
       " 'test': \"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False\\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True\\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False\\n    assert candidate([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True\\n    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True\\n    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False\\n\\n\"}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(file[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = json.loads(file[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "METADATA = {\n",
      "    'author': 'jt',\n",
      "    'dataset': 'test'\n",
      "}\n",
      "\n",
      "\n",
      "def check(candidate):\n",
      "    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\n",
      "    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False\n",
      "    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True\n",
      "    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False\n",
      "    assert candidate([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True\n",
      "    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True\n",
      "    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(q1['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from typing import List\n",
      "\n",
      "\n",
      "def separate_paren_groups(paren_string: str) -> List[str]:\n",
      "    \"\"\" Input to this function is a string containing multiple groups of nested parentheses. Your goal is to\n",
      "    separate those group into separate strings and return the list of those.\n",
      "    Separate groups are balanced (each open brace is properly closed) and not nested within each other\n",
      "    Ignore any spaces in the input string.\n",
      "    >>> separate_paren_groups('( ) (( )) (( )( ))')\n",
      "    ['()', '(())', '(()())']\n",
      "    \"\"\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(q1['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  requests\n",
    "def find_examples(docstrings: str, num_examples: int=10) -> list:\n",
    "    \"\"\"\n",
    "    Searches example database for most relevant examples to aid in code generation\n",
    "    \"\"\"\n",
    "    # currently hit local running bm25 database\n",
    "    url = \"http://127.0.0.1:8000/get_answers\"\n",
    "    data = {\"query\": docstrings, \"n_answers\": num_examples}\n",
    "    r = requests.post(url=url, json=data)\n",
    "    answers = r.json()[\"answers\"]\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from execution import check_correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jackweissenberger/anaconda3/envs/spec-trans/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../codegen/350-mono-model were not used when initializing CodeGenForCausalLM: ['transformer.h.13.attn.bias', 'transformer.h.15.attn.bias', 'transformer.h.16.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.12.attn.bias', 'transformer.h.19.attn.bias', 'transformer.h.14.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.2.attn.bias', 'transformer.h.6.attn.bias', 'transformer.h.3.attn.bias', 'transformer.h.11.attn.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.5.attn.bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.10.attn.bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.9.attn.bias', 'transformer.h.18.attn.bias', 'transformer.h.17.attn.bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.0.attn.bias', 'transformer.h.4.attn.bias', 'transformer.h.1.attn.bias', 'transformer.h.8.attn.bias']\n",
      "- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CodeGenForCausalLM were not initialized from the model checkpoint at ../codegen/350-mono-model and are newly initialized: ['transformer.h.7.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.0.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.9.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.11.attn.causal_mask']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"../codegen/350-mono-tokenizer\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"../codegen/350-mono-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation_stopping_criteria import StoppingCriteria\n",
    "import torch\n",
    "class CodeOnlyWithinFunction(StoppingCriteria):\n",
    "    \"\"\"\n",
    "    This is the stopping criteria for code generation models such that they only generate code\n",
    "    that starts each line with a tab. This restrics the models from generating code outside\n",
    "    of a function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # newline: 198\n",
    "        # tab: 197\n",
    "        # \"    \": 50284\n",
    "        # 8 spaces: 50280\n",
    "        self.ok_tokens = {198, 197, 50284, 50280, 220}\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "                Indices of input sequence tokens in the vocabulary.\n",
    "\n",
    "                Indices can be obtained using [`BertTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
    "                [`PreTrainedTokenizer.__call__`] for details.\n",
    "\n",
    "                [What are input IDs?](../glossary#input-ids)\n",
    "            scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
    "                Prediction scores of a language modeling head. These can be scores for each vocabulary token before SoftMax\n",
    "                or scores for each vocabulary token after SoftMax.\n",
    "            kwargs:\n",
    "                Additional stopping criteria specific kwargs.\n",
    "\n",
    "        Return:\n",
    "            `bool`. `False` indicates we should continue, `True` indicates we should stop.\n",
    "        \"\"\"\n",
    "\n",
    "        # decode\n",
    "        splits = tokenizer.decode(input_ids[0]).split('\\n')\n",
    "\n",
    "        for i in reversed(range(len(splits))):\n",
    "            if splits[i] == '' or len(splits[i]) == 0:\n",
    "                continue\n",
    "            if splits[i].startswith('def '):\n",
    "                # this is the case where we have gotten back to the function defintion and can allow lines without tabs\n",
    "                return False\n",
    "            if not splits[i][0].isspace():\n",
    "                # stop generation\n",
    "                return True\n",
    "        \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf = CodeOnlyWithinFunction()\n",
    "cf(tokenizer(q1[\"prompt\"], return_tensors=\"pt\").input_ids, scores=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(prompt):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    generated_ids = model.generate(input_ids, max_length=1024, temperature=0.2, stopping_criteria=[cf], pad_token_id=2)\n",
    "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = model_inference(q1[\"prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from typing import List\n",
      "\n",
      "\n",
      "def separate_paren_groups(paren_string: str) -> List[str]:\n",
      "    \"\"\" Input to this function is a string containing multiple groups of nested parentheses. Your goal is to\n",
      "    separate those group into separate strings and return the list of those.\n",
      "    Separate groups are balanced (each open brace is properly closed) and not nested within each other\n",
      "    Ignore any spaces in the input string.\n",
      "    >>> separate_paren_groups('( ) (( )) (( )( ))')\n",
      "    ['()', '(())', '(()())']\n",
      "    \"\"\"\n",
      "    paren_groups = []\n",
      "    for i in range(len(paren_string)):\n",
      "        if paren_string[i] == '(':\n",
      "            paren_groups.append(paren_string[i:])\n",
      "        elif paren_string[i] == ')':\n",
      "            if len(paren_groups) > 0:\n",
      "                paren_groups.pop()\n",
      "            else:\n",
      "                return []\n",
      "    return paren_groups\n",
      "\n",
      "\n",
      "def\n"
     ]
    }
   ],
   "source": [
    "print(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_model_output(model_output):\n",
    "    splits = model_output.split('\\n')\n",
    "    for i in reversed(range(len(splits))):\n",
    "        if splits[i] == '' or len(splits[i]) == 0:\n",
    "            continue\n",
    "        if splits[i][0].isspace():\n",
    "            splits = splits[:i+1]\n",
    "            break\n",
    "    return '\\n'.join(splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanded_output = clean_up_model_output(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from typing import List\n",
      "\n",
      "\n",
      "def separate_paren_groups(paren_string: str) -> List[str]:\n",
      "    \"\"\" Input to this function is a string containing multiple groups of nested parentheses. Your goal is to\n",
      "    separate those group into separate strings and return the list of those.\n",
      "    Separate groups are balanced (each open brace is properly closed) and not nested within each other\n",
      "    Ignore any spaces in the input string.\n",
      "    >>> separate_paren_groups('( ) (( )) (( )( ))')\n",
      "    ['()', '(())', '(()())']\n",
      "    \"\"\"\n",
      "    paren_groups = []\n",
      "    for i in range(len(paren_string)):\n",
      "        if paren_string[i] == '(':\n",
      "            paren_groups.append(paren_string[i:])\n",
      "        elif paren_string[i] == ')':\n",
      "            if len(paren_groups) > 0:\n",
      "                paren_groups.pop()\n",
      "            else:\n",
      "                return []\n",
      "    return paren_groups\n"
     ]
    }
   ],
   "source": [
    "print(cleanded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "METADATA = {\n",
      "    'author': 'jt',\n",
      "    'dataset': 'test'\n",
      "}\n",
      "\n",
      "\n",
      "def check(candidate):\n",
      "    assert candidate('(()()) ((())) () ((())()())') == [\n",
      "        '(()())', '((()))', '()', '((())()())'\n",
      "    ]\n",
      "    assert candidate('() (()) ((())) (((())))') == [\n",
      "        '()', '(())', '((()))', '(((())))'\n",
      "    ]\n",
      "    assert candidate('(()(())((())))') == [\n",
      "        '(()(())((())))'\n",
      "    ]\n",
      "    assert candidate('( ) (( )) (( )( ))') == ['()', '(())', '(()())']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(q1['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0756cb1984a5d775694d19959f964754a80a56723f950d1db60de6831f27d71c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('spec-trans')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
